import heapq
import math
from multiprocessing.dummy import Pool as ThreadPool
import string
from deep.dataloader.util import *



def greed(sentence, cr, deploy_model):
    """
    Each step get the most probable word until <END>.
    """
    minLen = 5
    maxLen = 20
    (x, x_mask) = cr.transform_input_data(sentence)
    print "original x: ", sentence
    currentLength = 0
    
    error = 0.0
    
    while True: 
        pred_w, pred_w_p = deploy_model(x, x_mask)
        pred_w_list = pred_w_p.flatten().tolist()
        
        sorted_index = heapq.nlargest(1, enumerate(pred_w_list), key=lambda s: s[1])
        p_word, p_word_prob = zip(*(sorted_index[:10]))
        # print 'currentLength %d:' % currentLength
        l1 = cr.transformInputText(p_word)
        l2 = p_word_prob
        # for ll1, ll2 in zip(l1, l2):
        #    print '%s\t%.8f' % (ll1, ll2)
        error -= numpy.log(l2)
        
        if currentLength < minLen:
            for c in p_word:
                choice = c
                if choice > 2:
                    break
        else:
            for c in p_word:
                choice = c
                if choice >= 2:
                    break
                
        x = numpy.concatenate([x, [[choice]]], axis=0)
        
        x_mask = numpy.concatenate([x_mask, [[1]]], axis=0)
        currentLength += 1
        if choice == 2 or currentLength > maxLen:
            break
#             print "pred: ", x
    return [x.flatten().tolist()[0]], error


def beam_search(sentence, cr, deploy_model, minLen=1, maxLen = 50, search_scope = 200,
                beam_size = 200, output_size = 20):
    """
    Each step get the most x probable word, Then explain the x word each step until <END>.
    """
    pool = ThreadPool(12)
    stop_tag = cr.get_word_dictionary()['<END>']
    available_flag = 10000000
   
    class SentenceScorePair(object):
        def __init__(self, priority, sentence):
            self.priority = priority
            self.sentence = sentence
        def __cmp__(self, other):
            return cmp(self.priority, \
                       other.priority)
    
    def step(last_sentence, score, pred_words_prob):
        pred_words_list = pred_words_prob.flatten().tolist()
        sorted_index = heapq.nlargest(search_scope, enumerate(pred_words_list), key=lambda s: s[1])
        results = list()
        cands = list()
        for pred_word, pred_word_prob in sorted_index:
            current_score = score - math.log(pred_word_prob)
            if available_flag < current_score:
                continue
            new_sentence = last_sentence + [pred_word]
            
            if pred_word == stop_tag:
                if len(new_sentence) - base_length < minLen:
                    continue
                cands.append(SentenceScorePair(score - math.log(pred_word_prob), new_sentence))
            else:
                results.append(SentenceScorePair(score - math.log(pred_word_prob), new_sentence))
        return results, cands
    
    # get question and question_mask
    (question, question_mask) = cr.transform_input_data(sentence)
    base_length = question.shape[0]
    (tanswer, tanswer_mask) = (question[-1:,:], question_mask[-1:,:])
    pred_word, pred_words_prob = deploy_model(question[:-1,:], question_mask[:-1,:], tanswer, tanswer_mask)
    sentence = load_sentence('', cr.word2index, special_flag=cr.special_flag)
    sQueue, candidates = step(sentence, 0, pred_words_prob)
    # search_scope = 1
    # iterative from current sentence get new sentence by step() 
    for iter in xrange(maxLen):
        current_len = len(sQueue)
        if current_len == 0:
            break
        buffer_Queue = list()
        candidate_list = [q.sentence for q in sQueue]
        current_len = len(candidate_list)
        
        tanswer, tanswer_mask = get_mask_data(candidate_list)
        ext_question = numpy.concatenate([question]*tanswer.shape[1], axis=1)
        ext_question_mask = numpy.concatenate([question_mask]*tanswer.shape[1], axis=1)
        pred_word, pred_words_prob = deploy_model(ext_question[:-1,:], ext_question_mask[:-1,],
                                                  tanswer, tanswer_mask)
        # multi thread
        sentences = pool.map(lambda i: step(sQueue[i].sentence, sQueue[i].priority,
                                            pred_words_prob[i]),
                             range(current_len))
        for sentence, priority in sentences:
            buffer_Queue.extend(sentence)
            candidates.extend(priority)
            
        if len(candidates) >= output_size:
            candidates = heapq.nsmallest(output_size, candidates)
            available_flag = candidates[-1].priority
        sQueue = buffer_Queue
        sQueue = heapq.nsmallest(beam_size, sQueue)
    pool.close()
    
    candidates = heapq.nsmallest(output_size, candidates, key=lambda x: 1.0 * x.priority)# / len(x.sentence))
    return [cand.sentence for cand in candidates], [cand.priority for cand in candidates]


def beam_search_t(sentence, cr, deploy_model, get_topic_pro,minLen=1, maxLen = 50, search_scope = 200,
                beam_size = 200, output_size = 20):
    """
    Each step get the most x probable word, Then explain the x word each step until <END>.
    """
    pool = ThreadPool(12)
    stop_tag = cr.get_word_dictionary()['<END>']
    available_flag = 10000000

    class SentenceScorePair(object):
        def __init__(self, priority, sentence):
            self.priority = priority
            self.sentence = sentence
        def __cmp__(self, other):
            return cmp(self.priority, \
                       other.priority)

    def step(last_sentence, score, pred_words_prob):
        pred_words_list = pred_words_prob.flatten().tolist()
        sorted_index = heapq.nlargest(search_scope, enumerate(pred_words_list), key=lambda s: s[1])
        results = list()
        cands = list()
        for pred_word, pred_word_prob in sorted_index:
            current_score = score - math.log(pred_word_prob)
            if available_flag < current_score:
                continue
            new_sentence = last_sentence + [pred_word]

            if pred_word == stop_tag:
                if len(new_sentence) - base_length < minLen:
                    continue
                cands.append(SentenceScorePair(score - math.log(pred_word_prob), new_sentence))
            else:
                results.append(SentenceScorePair(score - math.log(pred_word_prob), new_sentence))
        return results, cands

    # get question and question_mask
    #sentence_topic=sentence.strip().split('   ')
    sentence_topic=sentence.strip().split('\t')
    #print sentence.encode('gb18030')
    #print len(sentence_topic)
    topic_label=[[string.atoi(sentence_topic[1])]]
    sentence=sentence_topic[0]
    (question, question_mask) = cr.transform_input_data(sentence)
    answer_topic_pro=get_topic_pro(question,question_mask)
    answer_topic_pro_list=heapq.nlargest(search_scope,enumerate(answer_topic_pro[0]),key=lambda s: s[1])
    candidates_all=[]
    candidates_all.append([])
    candidates_all.append([])
    topic_num_thre=3
    topic_num_epoch=0
    for topic_id,topic_pro in answer_topic_pro_list:
        '''
        if(topic_id==169):
            continue
        '''
        topic_num_epoch+=1
        #print topic_num_epoch
        if(topic_num_epoch>topic_num_thre or (topic_num_epoch>1 and topic_pro<0.1)):
            break
        print topic_num_epoch
        if(1):
            if(string.atoi(sentence_topic[1])==-1):
                topic_label=[[topic_id]]
            topic_label=[[169]]
            available_flag = 10000000
            sentence=sentence_topic[0]
            (question, question_mask) = cr.transform_input_data(sentence)
            base_length = question.shape[0]
            (tanswer, tanswer_mask) = (question[-1:,:], question_mask[-1:,:])
            pred_word, pred_words_prob = deploy_model(question[:-1,:], question_mask[:-1,:], tanswer, tanswer_mask,topic_label)
            sentence = load_sentence('', cr.word2index, special_flag=cr.special_flag)
            sQueue, candidates = step(sentence, 0, pred_words_prob)
            # search_scope = 1
            # iterative from current sentence get new sentence by step()
            for iter in xrange(maxLen):
                current_len = len(sQueue)
                if current_len == 0:
                    break
                buffer_Queue = list()
                candidate_list = [q.sentence for q in sQueue]
                current_len = len(candidate_list)

                tanswer, tanswer_mask = get_mask_data(candidate_list)
                ext_question = numpy.concatenate([question]*tanswer.shape[1], axis=1)
                ext_question_mask = numpy.concatenate([question_mask]*tanswer.shape[1], axis=1)
                ext_topic = numpy.concatenate([topic_label]*tanswer.shape[1], axis=1)
                pred_word, pred_words_prob = deploy_model(ext_question[:-1,:], ext_question_mask[:-1,],
                                                          tanswer, tanswer_mask,ext_topic)
                # multi thread
                sentences = pool.map(lambda i: step(sQueue[i].sentence, sQueue[i].priority,
                                                    pred_words_prob[i]),
                                     range(current_len))
                for sentence, priority in sentences:
                    buffer_Queue.extend(sentence)
                    candidates.extend(priority)

                if len(candidates) >= output_size:
                    candidates = heapq.nsmallest(output_size, candidates)
                    available_flag = candidates[-1].priority
                sQueue = buffer_Queue
                sQueue = heapq.nsmallest(beam_size, sQueue)
            candidates = heapq.nsmallest(output_size, candidates, key=lambda x: 1.0 * x.priority)# / len(x.sentence))
            for cand in candidates:
                cand.priority-=math.log(topic_pro)
                cand.sentence.append(topic_id)
            #print topic_num_epoch,'\t',[cand.sentence for cand in candidates]
            candidates_all[0]+=[cand.sentence for cand in candidates]
            candidates_all[1]+=[cand.priority for cand in candidates]
    pool.close()
    return candidates_all[0], candidates_all[1]
